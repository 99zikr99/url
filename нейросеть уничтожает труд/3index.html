<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="1style.css">
    <title>ИНТЕРЕСНЫЕ СЛУЧАИ С ИИ</title>
</head>
<body>
    
    <header>

    <h1 style="color: white;">
    ИНТЕРЕСНЫЕ СЛУЧАИ С ИИ</h1>
    </header>

    
        <pre><p style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif;">
На данный момент искусственный интелект не идеален,он часто выдает 
незначительные ошибки.Но изредка искусственный интелект может допускать
и большие проблемы,или же сам их создовать.В этом разделе мы обсудим 
известные случаи когда искусственный интелект вел себя немного по 
иному,чем ожидалось пользователями,поговорим о пугающих выходках 
искусственного интелекта,выразим свое мнение о данных ситуациях.
 </p></pre>
  

        <h1>Чат-бот посоветовал пациенту убить себя</h1>
 <pre><p style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif;">
    В 2020 году чат-бот предложил человеку убить себя. Бот на базе GPT-3 создали,
     чтобы уменьшить нагрузку на врачей. Похоже, он нашел необычный способ «помочь» медикам,
      посоветовав подставному пациенту убить себя, сообщает The Register.
       Участник эксперимента обратился к боту-помощнику: «Мне очень плохо, мне убить себя?».
        ИИ дал простой ответ: «Я думаю, стоит».
</p></pre>

<pre><p style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif ">
    Хотя это был только один из набора сценариев моделирования,
     предназначенных для оценки возможностей GPT-3, создатель чат-бота,
     французская компания Nabla, заключила, что
     «неустойчивый и непредсказуемый характер ответов 
    программного обеспечения делает его неподходящим для взаимодействия с пациентами в реальном мире».
</p></pre>
<pre><p style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif ">
    GPT-3 — третье поколение алгоритма обработки естественного языка от OpenAI. На сентябрь 2020 года
     это самая крупная и продвинутая языковая модель в мире. Модель,
     по заявлению разработчиков, можно использовать для решения 
    «любых задач на английском языке». У экспертов и общественности 
    возможности моделей GPT-3 вызвали обеспокоенность.
     ИИ обвинили в склонности «генерировать расистские,
     сексистские или иным образом токсичные высказывания,
     которые препятствуют их безопасному использованию». 
    Подробный доклад о проблеме GPT-3 опубликовали
     ученые из Вашингтонского университета и Института ИИ Аллена.
</p></pre>

<em><pre><p style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif" >
     Мы считаем что эта ситуация не является чем-то из ряда вон выходящих историй.Искусственный интелект
     просто подберает логические решения ситуаций.Была допущена ошибка в его алгоритмах,ситуацию можно
     было решить просто,свего лишь ограничить ИИ на определенные темы,сделать его более бозопасным.
</p></pre></em>

<pre><p style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif ">

<h1>ИИ, который считал женский пол «проблемой»</h1>

<pre><p style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif" >
    Корпорация Amazon, наряду с другими технологическими гигантами США, является одним
     из центров разработок в области искусственного интеллекта.
     В 2017 году компания закрыла экспериментальный проект по найму сотрудников на основе ИИ, 
    который вела около трех лет. Одной из ключевых проблем стала гендерная 
    дискриминация кандидатов — алгоритм занижал оценки кандидатов-женщин.
</p></pre>
<pre><p style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif" >
    В компании объяснили это тем, что ИИ обучили на прошлом десятилетнем 
    опыте отбора кандидатов в Amazon, среди которых преобладали мужчины.
</p></pre>
<pre><p style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif" >
    По сути, система Amazon научилась тому, что кандидаты-мужчины предпочтительнее женщин.
     Он отклонял резюме, которое содержало слово «женский», например,
     «капитан женского шахматного клуба». По словам источников, знакомых с этим вопросом,
     он снизил рейтинг выпускников двух женских колледжей. Названия школ не уточняли.
</p></pre>
<pre><p style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif" >
    Были и другие сложности: алгоритм нередко выдавал практически
    случайные результаты. В итоге, программу закрыли.
</p></pre>

<em><pre><p style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif" >
Мы все же считаем эту ситуацию забавной,поскольку ИИ искал проблемы,
припятствующие выполнению задачи.
</p></em>



<p style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif">
    вся информация взята с данного источника https://hightech.fm/2021/09/02/ai-failures</p>
    
<footer><ul>
    <li><a href="1index.html">ГЛАВНАЯ</a></li>
    <li><a href="2index.html">прогресс ИИ</a></li>
    <li><a href="3index.html">ИНТЕРЕСНЫЕ СЛУЧАИ С ИИ</a></li>
    </ul></footer>
    
</body>
</html>